{"cells":[{"cell_type":"markdown","metadata":{"id":"VhwX6K1GGbYu"},"source":["# FinAI Summit 2025 - Creating and accelerating ML Solutions with Mercury\n","\n","[Mercury](https://www.bbvaaifactory.com/mercury/) is a modular Python library for Machine Learning and Data Science, developed at **BBVA**. It provides a comprehensive suite of tools designed to streamline and accelerate the creation of ML models, saving valuable development time while offering advanced Data Science functionality. Initially developed as [Inner Source](https://www.bbvaaifactory.com/mercury-acelerando-la-reutilizacion-en-ciencia-de-datos-dentro-de-bbva/), several modules of Mercury have been released as open source.\n","\n","This notebook demonstrates how easily you can use Mercury to enhance your ML workflows. Whether you're validating datasets, running robust tests, explaining models, or monitoring data drift, Mercury provides an effective solution for each task. In this notebook, we will explore the following modules:\n","\n","- **mercury-dataschema**: Ensures consistency by validating whether different datasets conform to the same schema.\n","- **mercury-robust**: Provides robust testing for ML models and datasets, ensuring reliability.\n","- **mercury-explainability**: Offers tools to interpret ML models, helping you understand model decisions.\n","- **mercury-monitoring**: Monitors models and data in production environments, detecting issues such as data drift.\n","\n","## Why Use Mercury?\n","\n","By leveraging Mercury, you'll experience:\n","\n","- **Ease of use**: Its intuitive modular design allow seamless integration into your projects.\n","- **Faster development**: Pre-built components let you focus on building models rather than on redundant tasks.\n","- **Advanced functionality**: Gain access to tools for [schema validation](https://bbva.github.io/mercury-dataschema), [model explainability](https://bbva.github.io/mercury-explainability), [event sequence analysis](https://bbva.github.io/mercury-reels), [subset querying](https://bbva.github.io/mercury-settrie), [monitoring](https://bbva.github.io/mercury-monitoring) of models and data, and [robust testing](https://bbva.github.io/mercury-robust).\n","\n","## Try it Yourself!\n","\n","Explore the code below and modify it to fit your specific use cases. Whether you're validating data or explaining models, you'll find how Mercury simplifies your workflow.\n","\n","Letâ€™s dive in!"]},{"cell_type":"markdown","metadata":{"id":"bL2WI0ElGhuJ"},"source":["## Install Mercury and Setup\n","\n","The next cell install the Mercury libraries that we will use. Additionally, we install alibi which is used in mercury-explainability\n","\n","You may need to restart the kernel after the installation"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eptpU7PUGdp7"},"outputs":[],"source":["# Suppressing error messages as they are related to dependency warnings of packages preinstalled by Kaggle in the environment and not used in the notebook\n","!pip install -U mercury-dataschema mercury-robust mercury-monitoring mercury-explainability alibi 2>/dev/null\n","#!pip install -U ipywidgets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GK6JocggHDsQ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import roc_auc_score\n","\n","\n","random_state= 232\n","n_sample = 100000\n","\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.max_rows', 150)"]},{"cell_type":"markdown","metadata":{"id":"CHnup6Q1HINn"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0q7XD4mHHbJ"},"outputs":[],"source":["df_url = \"https://raw.githubusercontent.com/BBVA/mercury/refs/heads/master/src/data/finai_summit_2025/dataset.csv\"\n","df = pd.read_csv(df_url)"]},{"cell_type":"markdown","metadata":{"id":"nq-ioyx7HP5y"},"source":["## Preprocessing\n","\n","Now, we apply some basic preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25xCeO1sHOYj"},"outputs":[],"source":["def apply_preprocessing(df):\n","    # Clean features with high number of nulls\n","    threshold_nulls = 0.25\n","\n","    for f in df.columns:\n","        percent_nulls_f = df[f].isnull().sum() / len(df)\n","        if percent_nulls_f > threshold_nulls:\n","            df = df.drop(f, axis=1)\n","\n","\n","    return df\n","\n","df = apply_preprocessing(df)\n","\n","# Drop no feature columns\n","df = df.drop(['id', 'time'], axis=1)\n","\n","label_col = \"target\"\n","feature_cols = [c for c in df.columns if c!=label_col]"]},{"cell_type":"markdown","metadata":{"id":"PkzY88IZHsfx"},"source":["### Train / Test Split\n","\n","We split the dataset in train and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jM4z7RlHrSi"},"outputs":[],"source":["df = df.sample(n=n_sample, random_state=random_state)\n","\n","df_train, df_test = train_test_split(df, test_size=0.25, random_state=random_state)"]},{"cell_type":"markdown","metadata":{"id":"_rstd2ndICq3"},"source":["## Mercury-Dataschema\n","\n","[Mercury-dataschema](https://bbva.github.io/mercury-dataschema/) is a utility tool that can auto-infer feature types and calculate different statistics. It is also used in the mercury-robust submodule. In this case, we will create a dataschema to use it later when creating the robust tests with mercury-robust.\n","\n","Because we had the description of the features, we will manually specify the feature types instead of using the auto-inference.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WBtBB5SIB6Z"},"outputs":[],"source":["from mercury.dataschema.schemagen import DataSchema\n","\n","categorical_feats = ['type_1']\n","discrete_feats = ['num_past_requests_1', 'num_past_requests_2', 'count_1', 'count_2', 'count_3', 'count_4', 'count_5', 'num_1', 'num_2', 'num_3', 'num_4']\n","binary_feats = ['type_2']\n","continuos_feats = ['payment_1', 'payment_2', 'quantity_1', 'pendent_quantity_1', 'pendent_quanitty_2', 'quantity_2', 'rate_1', 'rate_2', 'quantity_3', 'quantity_4', 'quantity_5', 'amount_1']\n","\n","schema = DataSchema().generate_manual(\n","    df_train,\n","    categ_columns=categorical_feats,\n","    discrete_columns=discrete_feats,\n","    binary_columns=binary_feats\n",")"]},{"cell_type":"markdown","metadata":{"id":"mUy31ZtCJM3k"},"source":["## Further preprocessing\n","\n","The next cell transform the categorical features. We will just use an `OrdinalEncoder` for simplicity, but other approaches might be more appropriate in this case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mC2-TEl9JNUo"},"outputs":[],"source":["encoders = {}\n","for c in df_train[categorical_feats + binary_feats]:\n","    encoders[c] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan).fit(df_train[c].values.reshape(-1, 1))\n","    df_train[c] = encoders[c].transform(df_train[c].values.reshape(-1, 1))\n","    df_test[c] = encoders[c].transform(df_test[c].values.reshape(-1, 1))"]},{"cell_type":"markdown","metadata":{"id":"OM7qmelDJQh4"},"source":["### Replace nulls\n","\n","Now we remove the remaining nulls values. Once again, we're using a simple method, but this could be replaced by a more sophisticated approach."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZCODmA-JPM4"},"outputs":[],"source":["for f in schema.categorical_feats:\n","    df_train[f] = df_train[f].fillna(df_train[f].mode().values[0])\n","    df_test[f] = df_test[f].fillna(df_train[f].mode().values[0])\n","\n","for f in schema.binary_feats:\n","    df_train[f] = df_train[f].fillna(df_train[f].mode().values[0])\n","    df_test[f] = df_test[f].fillna(df_train[f].mode().values[0])\n","\n","for f in schema.discrete_feats:\n","    df_train[f] = df_train[f].fillna(df_train[f].median())\n","    df_test[f] = df_test[f].fillna(df_train[f].median())\n","\n","for f in schema.continuous_feats:\n","    df_train[f] = df_train[f].fillna(df_train[f].mean())\n","    df_test[f] = df_test[f].fillna(df_train[f].mean())"]},{"cell_type":"markdown","metadata":{"id":"eTD8EB2SJWT1"},"source":["## Robust Test Dataset\n","\n","Let's proceed now to test the dataset that we created with [mercury-robust](https://bbva.github.io/mercury-robust/). Mercury robust allows us to create test suites in order to test different aspects of the dataset.\n","For example, you can create and run a `TestSuite` with just one test the next code:\n","\n","```python\n","from mercury.robust.data_test import LinearCombinationsTest\n","from mercury.robust import TestSuite\n","\n","linear_comb_test = LinearCombinationsTest(df_trian, dataset_schema=schma_reference)\n","\n","test_suite = TestSuite(tests=[linear_comb_test])\n","test_suite.run()\n","\n","test_suite.get_results_as_df()\n","```\n","\n","\n","Create in the next cell a TestSuite with the next tests:\n","\n","- LinearCombinationTest: Check if linear combinations exist in our dataset.\n","- LabelLeakingTest: Check that no feature leaks information about the target variable.\n","- NoisyLabelsTest: Guarantee that the labels in our dataset have a minimum quality.\n","- NoDuplicatesTest: Check that we do not have repeated samples in the dataset.\n","- SampleLeakingTest: Check that our test dataset does not contain samples that are already included in the training dataset.\n","\n","Then run the test suite and check which tests are failing. You can check the [documentation](https://bbva.github.io/mercury-robust/site/reference/data_tests/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcGzHdozJTyl"},"outputs":[],"source":["# WRITE YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"MZE6L_2RJjBj"},"source":["Which Tests are Failing?"]},{"cell_type":"markdown","metadata":{"id":"5VwK7KQhKAcB"},"source":["## Train the model\n","\n","Let's train a model. In this case, we'll train a Decision Tree, leaving all the parameters at their default values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6Uq_IVtKCHg"},"outputs":[],"source":["num_feats = discrete_feats + continuos_feats\n","cat_feats = categorical_feats + binary_feats\n","\n","model_1 = DecisionTreeClassifier()\n","\n","model_1 = model_1.fit(df_train[num_feats + cat_feats], df_train[label_col])"]},{"cell_type":"markdown","metadata":{"id":"gQB1670-J0yd"},"source":["## Robust Model Tests\n","\n","We can also create test suites to test our models. In the next cell, create a test suite with:\n","\n","- ModelSimplicityChecker: To check if a simpler model gives similar or better performance.\n","- TreeCoverageTest: To check that when we apply a test set to the trained decision tree, we cover at least a percentage of the branches.\n","\n","You can check the [documentation](https://bbva.github.io/mercury-robust/site/reference/model_tests/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7orvWqrzJh3P"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO CREATE AND RUN A TEST SUITE"]},{"cell_type":"markdown","metadata":{"id":"Za0sST6dKVbr"},"source":["Which test are failing?"]},{"cell_type":"markdown","metadata":{"id":"t5rhzJ4MLWZ1"},"source":["#### Train a second model\n","\n","In the next cell, try to train a new model that passes the tests failing before. Then execute again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqSgLDDCKWgu"},"outputs":[],"source":["#Â WRITE YOUR CODE TO TRAIN A NEW MODEL\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edWkPie7LYBg"},"outputs":[],"source":["# WRITE YOUR CODE TO CREATE AND RUN A TEST SUITE FOR THE NEW MODEL\n"]},{"cell_type":"markdown","metadata":{"id":"RjKNJyXvLlXl"},"source":["## Mercury-Explainability\n","\n","[Mercury-explainability](https://bbva.github.io/mercury-explainability/) provides several tools to help interpret ML models. Let's apply a couple of these components to gain deeper insights into how our model makes predictions and which features are most influential. Understanding the inner workings of the model will allow us to identify potential biases, assess the importance of features, and ensure the model's decisions align with expectations."]},{"cell_type":"markdown","metadata":{"id":"dWtQGow3Lqz2"},"source":["### CounterFactualExplainer\n","\n","First, we will use the `CounterFactualExplainer`, which is a local explainability method, ie. it tries to explain individual predictions. This method looks for necessary changes in the inputs of a given instance so that the model prediction is an output predefined by us instead of the actual prediction.\n","\n","\n","First, we train a new model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7palFEsLpH5"},"outputs":[],"source":["model_3 = DecisionTreeClassifier(class_weight='balanced', max_depth=6)\n","num_feats_short = num_feats[0:5]\n","cat_feats_short = cat_feats[0:5]\n","\n","\n","model_3 = model_3.fit(df_train[num_feats_short + cat_feats_short].to_numpy(), df_train[label_col])"]},{"cell_type":"markdown","metadata":{"id":"WphcNeycLwZj"},"source":["In the previous cell, we have limited the number of features to the first 5 (num_feats[0:5] for numerical features and cat_feats[0:5] for categorical features). This is done to simplify the model's graphical representation, making it easier to visualize and understand the structure without overwhelming detail. By reducing the feature set, we can focus on a smaller subset of data, which helps in producing more interpretable visual outputs.\n","\n","In the next cell, create a `CounterFactualExplainerBasic` object. You can do in the next way:\n","```python\n","from mercury.explainability import CounterFactualExplainerBasic\n","\n","counterfactual_basic = CounterFactualExplainerBasic(\n","  your_training_dataset,\n","  your_predict_method\n",")\n","```\n","You can check the [documentation](https://bbva.github.io/mercury-explainability/site/reference/explainers/#mercury.explainability.explainers.counter_fact_basic.CounterFactualExplainerBasic) for further details"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GimRonY2LugH"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO CREATE THE CounterFactualExplainerBasic"]},{"cell_type":"markdown","metadata":{"id":"oREQOeU97dK7"},"source":["Let's check what is the prediction of our second instance in the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Z9UMsVcL0ly"},"outputs":[],"source":["model_3.predict_proba([df_test[num_feats_short + cat_feats_short].iloc[2].values])"]},{"cell_type":"markdown","metadata":{"id":"ojKGcG6aMANS"},"source":["Now, let's use our explainer to explain our second instance. We want to know which changes we need in the inputs of our instance in order to change the prediction:\n","```python\n","explanation = counterfactual_basic.explain(\n","  our_instance_to_explain,\n","  thresold= # probability that we want our instance to reach\n","  class_idx= # class that we are specifying the probability in threshold argument\n","  keep_explored_points=False\n",")\n","```\n","\n","Then, you can show the explanation with `explanation.show()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Shk0Hk1PL1sx"},"outputs":[],"source":["#Â WRITE YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"yrfav67kMSYP"},"source":["What variables do need to change in order to change the class prediction?"]},{"cell_type":"markdown","metadata":{"id":"nAP2W-yAFFN0"},"source":["Now, let's create another model for the next explainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnMNk1lrFEAT"},"outputs":[],"source":["model_4 = DecisionTreeClassifier(\n","    class_weight='balanced', max_depth=6\n",")\n","\n","model_4 = model_4.fit(df_train[num_feats + cat_feats].to_numpy(), df_train[label_col])"]},{"cell_type":"markdown","metadata":{"id":"NIUE-YXaNdsb"},"source":["### ALE Plots\n","\n","ALE plots show how model inputs affect the prediction on average. ALE Plots tend to be more reliable than Partial Dependence Plots in cases with correlations between different inputs.\n","\n","In the next cell, use the `ALEExplainer` to create the ALE Plots. Check the documentation [here](https://bbva.github.io/mercury-explainability/site/reference/explainers/#mercury.explainability.explainers.ale.ALEExplainer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbLmy6VoE_GQ"},"outputs":[],"source":["#Â WRITE YOUR CODE HERE TO CREATE  The ALEEXplainer"]},{"cell_type":"markdown","metadata":{"id":"Jqx113IINlUw"},"source":["Once the `ALEExplainer` is created, call the method `explain` to obtain the explanation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDJ2H9sDNgaB"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO OBTAIN THE EXPLANATION"]},{"cell_type":"markdown","metadata":{"id":"Ae_OLmDxOH-K"},"source":["Now let's show the ALE plots. By selecting targets=[1] we indicate to show how changes the probability of default depending on the feature values\n","\n","Finally, you need to use the [`plot_ale`](https://bbva.github.io/mercury-explainability/site/reference/explainers/#mercury.explainability.explainers.ale.plot_ale) function to show the plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEXNfsJNN083"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO SHOW THE PLOTS"]},{"cell_type":"markdown","metadata":{"id":"7ostEeJWOOAV"},"source":["How are the different features impacting the model output?"]},{"cell_type":"markdown","metadata":{"id":"KtmEXydtOTq3"},"source":["## Mercury-monitoring\n","\n","[Mercury-monitoring](https://bbva.github.io/mercury-monitoring/) allows us to monitor data and model drift. In this case, we will use it to detect if there are changes in the input distribution.\n","\n","Let's read the dataset again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekP4EwOeOPKb"},"outputs":[],"source":["df = pd.read_csv(\"https://raw.githubusercontent.com/BBVA/mercury/refs/heads/master/src/data/finai_summit_2025/dataset.csv\")\n","df = apply_preprocessing(df)\n","\n","for f in schema.categorical_feats:\n","    df[f] = df[f].fillna(df[f].mode().values[0])\n","\n","for f in schema.binary_feats:\n","    df[f] = df[f].fillna(df[f].mode().values[0])\n","\n","for f in schema.discrete_feats:\n","    df[f] = df[f].fillna(df[f].median())\n","\n","for f in schema.continuous_feats:\n","    df[f] = df[f].fillna(df[f].mean())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxWnlFosOboH"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"F9FQDyAXOdfn"},"source":["We will check first check if we have drift between the time 21 and the time 90:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9IKNikaOd9E"},"outputs":[],"source":["df_time_21 = df[df[\"time\"] == 21]\n","df_time_90 = df[df[\"time\"] == 90]"]},{"cell_type":"markdown","metadata":{"id":"biDP7HtSOlXd"},"source":["### KS Drift\n","\n","Let's check if we have data drift between time 21 and time 90. We will use the [`KSDrift`](https://bbva.github.io/mercury-monitoring/site/reference/drift/#mercury.monitoring.drift.ks_drift_detector.KSDrift), which allows to detect drift by calculating the Kolmogorov-Smirnov (KS) statistic. For each feature in the datasets, a KS test is performed. The method `calculate_drift()` returns a dictionary with different metrics. The key `drift_detected` is a boolean  indicating if drift was detected. The key `score` contains the average of all KS statistic computed for all features and it can be used as a measure to track drift.\n","\n","Write the code in the next cell to calculate the data drift with the `KSDrift` detector. You can see more details in the [documentation](https://bbva.github.io/mercury-monitoring/site/reference/drift/#mercury.monitoring.drift.ks_drift_detector.KSDrift)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cEKGC9vOkh_"},"outputs":[],"source":["X_source = df_time_21[num_feats].to_numpy()\n","X_target = df_time_90[num_feats].to_numpy()\n","\n","# WRITE YOUR CODE HERE TO CALCULATE THE DATA DRIFT WITH KSDrift"]},{"cell_type":"markdown","metadata":{"id":"leG2O_BZSbVP"},"source":["Do we have data drift? What is the amount of drift score?"]},{"cell_type":"markdown","metadata":{"id":"gnzOD8u7Oxyo"},"source":["If we want to know which are the features with data drift, we can use the [`get_drifted_features`](https://bbva.github.io/mercury-monitoring/site/reference/drift/#mercury.monitoring.drift.base.BaseBatchDriftDetector.get_drifted_features) method. Use the next cell to obtain the features with drift\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-3lZLe0OyIq"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO GET THE DRIFTED FEATURES"]},{"cell_type":"markdown","metadata":{"id":"AphwOq6zO9zq"},"source":["With the method [`plot_feature_drift_scores`](https://bbva.github.io/mercury-monitoring/site/reference/drift/#mercury.monitoring.drift.base.BaseBatchDriftDetector.plot_feature_drift_scores) we can easily create a plot with the drift score for each feature.\n","\n","Use the next cell to plot the feature drift scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snGIXGW_Ozxh"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO PLOT THE FEATURE DRIFT SCORES"]},{"cell_type":"markdown","metadata":{"id":"GQuwfSXIPk9_"},"source":["Which are the features with higher drift?"]},{"cell_type":"markdown","metadata":{"id":"ETX23mnCPnHu"},"source":["### DomainClassifier Drift\n","\n","Now, we will use the [`DomainClassifierDrift`](https://bbva.github.io/mercury-monitoring/site/reference/drift/#mercury.monitoring.drift.domain_classifier_drift_detector). It works similarly to the `KSDrift`, but in this case, it trains a classifier (Random Forest) to distinguish between a source dataset and a target dataset. If the better the classifier performance, the higher the data drift between both datasets.\n","\n","Write the code in the next cell to calculate the data drift with the `DomainClassifierDrift` detector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5nQOzqHPlRQ"},"outputs":[],"source":["X_source = df_time_21[num_feats].dropna().to_numpy()\n","X_target = df_time_90[num_feats].dropna().to_numpy()\n","\n","# WRITE YOUR CODE HERE TO CALCULATE THE DATA DRIFT WITH DomainClassifierDrift"]},{"cell_type":"markdown","metadata":{"id":"YJzLWoXrQQ0U"},"source":["We can see that drift was detected"]},{"cell_type":"markdown","metadata":{"id":"2D1Fk4ZaQS-0"},"source":["### Drift over time\n","\n","Now, let's use the KSDrift to track drift overtime. We will consider our initial dataset the data previous to time 50, as calculate the drift for the following times"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xckMb-Y5QPIo"},"outputs":[],"source":["df_train = df[df[\"time\"] <= 70].copy()\n","df_inference = df[df[\"time\"] > 70].copy()\n","\n","print(len(df_train))\n","print(len(df_inference))"]},{"cell_type":"markdown","metadata":{"id":"ElC9FQTdVdWM"},"source":["Then, you can calculate and save the drift at each `t` and plot later"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7a9cWWQQVA6"},"outputs":[],"source":["drift_scores = []\n","\n","for t in df_inference[\"time\"].sort_values().unique():\n","\n","    X_source = df_train[num_feats].to_numpy()\n","    X_target = df_inference[df_inference[\"time\"] == t][num_feats].to_numpy()\n","\n","    # WRITE THE CODE HERE TO CALCULATE THE DRIFT WITH THE KSDRIFT AND KEEP IT IN drift_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ufu-OnSNQWij"},"outputs":[],"source":["# WRITE YOUR CODE HERE TO PLOT THE DATA DRIFT OVER TIME"]},{"cell_type":"markdown","metadata":{"id":"uOz80WfpQgmm"},"source":["Is the data drift increasing over time?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
